{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "filled-panel",
   "metadata": {},
   "source": [
    "# Redes neurais artificiais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-membrane",
   "metadata": {},
   "source": [
    "## Importando as libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chronic-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-delicious",
   "metadata": {},
   "source": [
    "## Base credit data - Resultado da an√°lise: 99.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-laptop",
   "metadata": {},
   "source": [
    "### Importando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assumed-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/credit.pkl', 'rb') as f:\n",
    "    X_train_credit, y_train_credit, X_test_credit, y_test_credit = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "civil-stock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_credit.shape, y_train_credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "czech-belle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_credit.shape, y_test_credit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-candle",
   "metadata": {},
   "source": [
    "### Experimentando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "armed-embassy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c7fjRs-qePa",
    "outputId": "8d128ce5-4ed1-46de-a8ae-6af2361a1ba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3 + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "curious-listening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61665823\n",
      "Iteration 2, loss = 0.57113521\n",
      "Iteration 3, loss = 0.52997536\n",
      "Iteration 4, loss = 0.49229320\n",
      "Iteration 5, loss = 0.45662785\n",
      "Iteration 6, loss = 0.42343423\n",
      "Iteration 7, loss = 0.39189538\n",
      "Iteration 8, loss = 0.36212424\n",
      "Iteration 9, loss = 0.33359788\n",
      "Iteration 10, loss = 0.30693059\n",
      "Iteration 11, loss = 0.28166528\n",
      "Iteration 12, loss = 0.25806895\n",
      "Iteration 13, loss = 0.23640450\n",
      "Iteration 14, loss = 0.21674877\n",
      "Iteration 15, loss = 0.19915058\n",
      "Iteration 16, loss = 0.18402375\n",
      "Iteration 17, loss = 0.17059686\n",
      "Iteration 18, loss = 0.15919046\n",
      "Iteration 19, loss = 0.14931633\n",
      "Iteration 20, loss = 0.14051061\n",
      "Iteration 21, loss = 0.13290175\n",
      "Iteration 22, loss = 0.12618220\n",
      "Iteration 23, loss = 0.12021612\n",
      "Iteration 24, loss = 0.11502133\n",
      "Iteration 25, loss = 0.11026686\n",
      "Iteration 26, loss = 0.10612757\n",
      "Iteration 27, loss = 0.10224988\n",
      "Iteration 28, loss = 0.09899107\n",
      "Iteration 29, loss = 0.09586845\n",
      "Iteration 30, loss = 0.09303866\n",
      "Iteration 31, loss = 0.09057424\n",
      "Iteration 32, loss = 0.08811231\n",
      "Iteration 33, loss = 0.08599794\n",
      "Iteration 34, loss = 0.08398782\n",
      "Iteration 35, loss = 0.08210581\n",
      "Iteration 36, loss = 0.08031334\n",
      "Iteration 37, loss = 0.07874742\n",
      "Iteration 38, loss = 0.07709008\n",
      "Iteration 39, loss = 0.07560743\n",
      "Iteration 40, loss = 0.07414531\n",
      "Iteration 41, loss = 0.07271810\n",
      "Iteration 42, loss = 0.07130924\n",
      "Iteration 43, loss = 0.07010284\n",
      "Iteration 44, loss = 0.06900002\n",
      "Iteration 45, loss = 0.06777658\n",
      "Iteration 46, loss = 0.06643317\n",
      "Iteration 47, loss = 0.06536053\n",
      "Iteration 48, loss = 0.06429550\n",
      "Iteration 49, loss = 0.06314320\n",
      "Iteration 50, loss = 0.06217414\n",
      "Iteration 51, loss = 0.06122567\n",
      "Iteration 52, loss = 0.06026094\n",
      "Iteration 53, loss = 0.05918881\n",
      "Iteration 54, loss = 0.05827902\n",
      "Iteration 55, loss = 0.05733267\n",
      "Iteration 56, loss = 0.05665541\n",
      "Iteration 57, loss = 0.05563248\n",
      "Iteration 58, loss = 0.05468679\n",
      "Iteration 59, loss = 0.05396464\n",
      "Iteration 60, loss = 0.05294432\n",
      "Iteration 61, loss = 0.05226529\n",
      "Iteration 62, loss = 0.05132658\n",
      "Iteration 63, loss = 0.05062689\n",
      "Iteration 64, loss = 0.04986809\n",
      "Iteration 65, loss = 0.04915861\n",
      "Iteration 66, loss = 0.04833992\n",
      "Iteration 67, loss = 0.04765091\n",
      "Iteration 68, loss = 0.04688512\n",
      "Iteration 69, loss = 0.04625558\n",
      "Iteration 70, loss = 0.04556756\n",
      "Iteration 71, loss = 0.04486864\n",
      "Iteration 72, loss = 0.04417908\n",
      "Iteration 73, loss = 0.04383416\n",
      "Iteration 74, loss = 0.04277292\n",
      "Iteration 75, loss = 0.04225812\n",
      "Iteration 76, loss = 0.04171416\n",
      "Iteration 77, loss = 0.04106493\n",
      "Iteration 78, loss = 0.04039194\n",
      "Iteration 79, loss = 0.03977042\n",
      "Iteration 80, loss = 0.03937923\n",
      "Iteration 81, loss = 0.03882010\n",
      "Iteration 82, loss = 0.03820531\n",
      "Iteration 83, loss = 0.03784481\n",
      "Iteration 84, loss = 0.03728494\n",
      "Iteration 85, loss = 0.03669511\n",
      "Iteration 86, loss = 0.03631192\n",
      "Iteration 87, loss = 0.03578008\n",
      "Iteration 88, loss = 0.03528830\n",
      "Iteration 89, loss = 0.03499742\n",
      "Iteration 90, loss = 0.03446679\n",
      "Iteration 91, loss = 0.03401748\n",
      "Iteration 92, loss = 0.03358249\n",
      "Iteration 93, loss = 0.03314258\n",
      "Iteration 94, loss = 0.03280696\n",
      "Iteration 95, loss = 0.03242370\n",
      "Iteration 96, loss = 0.03202052\n",
      "Iteration 97, loss = 0.03166389\n",
      "Iteration 98, loss = 0.03121100\n",
      "Iteration 99, loss = 0.03097290\n",
      "Iteration 100, loss = 0.03053070\n",
      "Iteration 101, loss = 0.03015960\n",
      "Iteration 102, loss = 0.02996817\n",
      "Iteration 103, loss = 0.02949491\n",
      "Iteration 104, loss = 0.02914478\n",
      "Iteration 105, loss = 0.02883086\n",
      "Iteration 106, loss = 0.02856759\n",
      "Iteration 107, loss = 0.02817649\n",
      "Iteration 108, loss = 0.02786712\n",
      "Iteration 109, loss = 0.02778086\n",
      "Iteration 110, loss = 0.02731978\n",
      "Iteration 111, loss = 0.02706703\n",
      "Iteration 112, loss = 0.02671710\n",
      "Iteration 113, loss = 0.02630935\n",
      "Iteration 114, loss = 0.02608382\n",
      "Iteration 115, loss = 0.02572178\n",
      "Iteration 116, loss = 0.02554616\n",
      "Iteration 117, loss = 0.02521515\n",
      "Iteration 118, loss = 0.02478602\n",
      "Iteration 119, loss = 0.02473847\n",
      "Iteration 120, loss = 0.02433488\n",
      "Iteration 121, loss = 0.02422053\n",
      "Iteration 122, loss = 0.02393194\n",
      "Iteration 123, loss = 0.02365190\n",
      "Iteration 124, loss = 0.02341741\n",
      "Iteration 125, loss = 0.02317442\n",
      "Iteration 126, loss = 0.02304442\n",
      "Iteration 127, loss = 0.02268526\n",
      "Iteration 128, loss = 0.02254551\n",
      "Iteration 129, loss = 0.02227215\n",
      "Iteration 130, loss = 0.02205508\n",
      "Iteration 131, loss = 0.02187955\n",
      "Iteration 132, loss = 0.02169342\n",
      "Iteration 133, loss = 0.02161643\n",
      "Iteration 134, loss = 0.02118347\n",
      "Iteration 135, loss = 0.02128196\n",
      "Iteration 136, loss = 0.02086621\n",
      "Iteration 137, loss = 0.02076092\n",
      "Iteration 138, loss = 0.02064083\n",
      "Iteration 139, loss = 0.02032947\n",
      "Iteration 140, loss = 0.02014325\n",
      "Iteration 141, loss = 0.01988691\n",
      "Iteration 142, loss = 0.01987884\n",
      "Iteration 143, loss = 0.01964604\n",
      "Iteration 144, loss = 0.01941414\n",
      "Iteration 145, loss = 0.01926238\n",
      "Iteration 146, loss = 0.01909652\n",
      "Iteration 147, loss = 0.01900142\n",
      "Iteration 148, loss = 0.01879290\n",
      "Iteration 149, loss = 0.01869301\n",
      "Iteration 150, loss = 0.01852242\n",
      "Iteration 151, loss = 0.01828826\n",
      "Iteration 152, loss = 0.01814511\n",
      "Iteration 153, loss = 0.01807140\n",
      "Iteration 154, loss = 0.01800842\n",
      "Iteration 155, loss = 0.01776635\n",
      "Iteration 156, loss = 0.01767852\n",
      "Iteration 157, loss = 0.01754543\n",
      "Iteration 158, loss = 0.01760369\n",
      "Iteration 159, loss = 0.01719746\n",
      "Iteration 160, loss = 0.01710829\n",
      "Iteration 161, loss = 0.01700656\n",
      "Iteration 162, loss = 0.01689413\n",
      "Iteration 163, loss = 0.01669964\n",
      "Iteration 164, loss = 0.01654553\n",
      "Iteration 165, loss = 0.01650822\n",
      "Iteration 166, loss = 0.01642112\n",
      "Iteration 167, loss = 0.01631194\n",
      "Iteration 168, loss = 0.01618220\n",
      "Iteration 169, loss = 0.01611497\n",
      "Iteration 170, loss = 0.01582564\n",
      "Iteration 171, loss = 0.01571203\n",
      "Iteration 172, loss = 0.01562295\n",
      "Iteration 173, loss = 0.01553088\n",
      "Iteration 174, loss = 0.01539233\n",
      "Iteration 175, loss = 0.01533063\n",
      "Iteration 176, loss = 0.01528079\n",
      "Iteration 177, loss = 0.01515316\n",
      "Iteration 178, loss = 0.01499760\n",
      "Iteration 179, loss = 0.01496490\n",
      "Iteration 180, loss = 0.01480626\n",
      "Iteration 181, loss = 0.01464134\n",
      "Iteration 182, loss = 0.01456146\n",
      "Iteration 183, loss = 0.01469003\n",
      "Iteration 184, loss = 0.01440495\n",
      "Iteration 185, loss = 0.01425713\n",
      "Iteration 186, loss = 0.01411609\n",
      "Iteration 187, loss = 0.01435233\n",
      "Iteration 188, loss = 0.01395810\n",
      "Iteration 189, loss = 0.01389262\n",
      "Iteration 190, loss = 0.01377502\n",
      "Iteration 191, loss = 0.01373910\n",
      "Iteration 192, loss = 0.01355271\n",
      "Iteration 193, loss = 0.01361594\n",
      "Iteration 194, loss = 0.01345273\n",
      "Iteration 195, loss = 0.01334589\n",
      "Iteration 196, loss = 0.01333675\n",
      "Iteration 197, loss = 0.01314443\n",
      "Iteration 198, loss = 0.01309321\n",
      "Iteration 199, loss = 0.01298664\n",
      "Iteration 200, loss = 0.01288843\n",
      "Iteration 201, loss = 0.01286906\n",
      "Iteration 202, loss = 0.01274120\n",
      "Iteration 203, loss = 0.01264197\n",
      "Iteration 204, loss = 0.01258134\n",
      "Iteration 205, loss = 0.01249784\n",
      "Iteration 206, loss = 0.01263858\n",
      "Iteration 207, loss = 0.01239749\n",
      "Iteration 208, loss = 0.01223041\n",
      "Iteration 209, loss = 0.01230385\n",
      "Iteration 210, loss = 0.01217636\n",
      "Iteration 211, loss = 0.01202946\n",
      "Iteration 212, loss = 0.01191964\n",
      "Iteration 213, loss = 0.01193843\n",
      "Iteration 214, loss = 0.01176422\n",
      "Iteration 215, loss = 0.01185718\n",
      "Iteration 216, loss = 0.01171832\n",
      "Iteration 217, loss = 0.01174637\n",
      "Iteration 218, loss = 0.01167690\n",
      "Iteration 219, loss = 0.01151919\n",
      "Iteration 220, loss = 0.01162063\n",
      "Iteration 221, loss = 0.01135929\n",
      "Iteration 222, loss = 0.01131780\n",
      "Iteration 223, loss = 0.01138932\n",
      "Iteration 224, loss = 0.01111010\n",
      "Iteration 225, loss = 0.01117799\n",
      "Iteration 226, loss = 0.01102976\n",
      "Iteration 227, loss = 0.01100500\n",
      "Iteration 228, loss = 0.01104320\n",
      "Iteration 229, loss = 0.01087311\n",
      "Iteration 230, loss = 0.01084006\n",
      "Iteration 231, loss = 0.01081808\n",
      "Iteration 232, loss = 0.01072084\n",
      "Iteration 233, loss = 0.01071574\n",
      "Iteration 234, loss = 0.01059839\n",
      "Iteration 235, loss = 0.01058538\n",
      "Iteration 236, loss = 0.01059596\n",
      "Iteration 237, loss = 0.01044829\n",
      "Iteration 238, loss = 0.01035081\n",
      "Iteration 239, loss = 0.01030124\n",
      "Iteration 240, loss = 0.01074920\n",
      "Iteration 241, loss = 0.01028879\n",
      "Iteration 242, loss = 0.01020387\n",
      "Iteration 243, loss = 0.01008269\n",
      "Iteration 244, loss = 0.01017070\n",
      "Iteration 245, loss = 0.01007444\n",
      "Iteration 246, loss = 0.00999317\n",
      "Iteration 247, loss = 0.01029519\n",
      "Iteration 248, loss = 0.00979665\n",
      "Iteration 249, loss = 0.00980523\n",
      "Iteration 250, loss = 0.00977858\n",
      "Iteration 251, loss = 0.00964233\n",
      "Iteration 252, loss = 0.00959785\n",
      "Iteration 253, loss = 0.00958230\n",
      "Iteration 254, loss = 0.00952411\n",
      "Iteration 255, loss = 0.00941580\n",
      "Iteration 256, loss = 0.00995800\n",
      "Iteration 257, loss = 0.00972397\n",
      "Iteration 258, loss = 0.00949115\n",
      "Iteration 259, loss = 0.00923343\n",
      "Iteration 260, loss = 0.00931211\n",
      "Iteration 261, loss = 0.00913042\n",
      "Iteration 262, loss = 0.00910916\n",
      "Iteration 263, loss = 0.00905830\n",
      "Iteration 264, loss = 0.00937062\n",
      "Iteration 265, loss = 0.00909757\n",
      "Iteration 266, loss = 0.00904682\n",
      "Iteration 267, loss = 0.00886350\n",
      "Iteration 268, loss = 0.00882239\n",
      "Iteration 269, loss = 0.00887151\n",
      "Iteration 270, loss = 0.00894120\n",
      "Iteration 271, loss = 0.00870104\n",
      "Iteration 272, loss = 0.00871035\n",
      "Iteration 273, loss = 0.00880523\n",
      "Iteration 274, loss = 0.00862364\n",
      "Iteration 275, loss = 0.00852774\n",
      "Iteration 276, loss = 0.00852943\n",
      "Iteration 277, loss = 0.00852873\n",
      "Iteration 278, loss = 0.00841082\n",
      "Iteration 279, loss = 0.00841414\n",
      "Iteration 280, loss = 0.00835405\n",
      "Iteration 281, loss = 0.00831809\n",
      "Iteration 282, loss = 0.00821709\n",
      "Iteration 283, loss = 0.00823077\n",
      "Iteration 284, loss = 0.00819498\n",
      "Iteration 285, loss = 0.00823209\n",
      "Iteration 286, loss = 0.00807844\n",
      "Iteration 287, loss = 0.00816884\n",
      "Iteration 288, loss = 0.00806063\n",
      "Iteration 289, loss = 0.00797194\n",
      "Iteration 290, loss = 0.00799263\n",
      "Iteration 291, loss = 0.00796993\n",
      "Iteration 292, loss = 0.00797061\n",
      "Iteration 293, loss = 0.00790526\n",
      "Iteration 294, loss = 0.00786891\n",
      "Iteration 295, loss = 0.00782132\n",
      "Iteration 296, loss = 0.00770417\n",
      "Iteration 297, loss = 0.00775552\n",
      "Iteration 298, loss = 0.00774097\n",
      "Iteration 299, loss = 0.00765768\n",
      "Iteration 300, loss = 0.00765576\n",
      "Iteration 301, loss = 0.00756999\n",
      "Iteration 302, loss = 0.00750351\n",
      "Iteration 303, loss = 0.00750978\n",
      "Iteration 304, loss = 0.00758210\n",
      "Iteration 305, loss = 0.00738026\n",
      "Iteration 306, loss = 0.00752879\n",
      "Iteration 307, loss = 0.00743632\n",
      "Iteration 308, loss = 0.00738739\n",
      "Iteration 309, loss = 0.00726225\n",
      "Iteration 310, loss = 0.00728668\n",
      "Iteration 311, loss = 0.00730837\n",
      "Iteration 312, loss = 0.00734326\n",
      "Iteration 313, loss = 0.00727430\n",
      "Iteration 314, loss = 0.00734100\n",
      "Iteration 315, loss = 0.00714996\n",
      "Iteration 316, loss = 0.00704324\n",
      "Iteration 317, loss = 0.00710135\n",
      "Iteration 318, loss = 0.00720795\n",
      "Iteration 319, loss = 0.00702987\n",
      "Iteration 320, loss = 0.00698248\n",
      "Iteration 321, loss = 0.00695551\n",
      "Iteration 322, loss = 0.00700497\n",
      "Iteration 323, loss = 0.00694595\n",
      "Iteration 324, loss = 0.00683988\n",
      "Iteration 325, loss = 0.00677900\n",
      "Iteration 326, loss = 0.00678881\n",
      "Iteration 327, loss = 0.00677021\n",
      "Iteration 328, loss = 0.00682603\n",
      "Iteration 329, loss = 0.00667300\n",
      "Iteration 330, loss = 0.00671586\n",
      "Iteration 331, loss = 0.00667909\n",
      "Iteration 332, loss = 0.00659105\n",
      "Iteration 333, loss = 0.00661313\n",
      "Iteration 334, loss = 0.00651562\n",
      "Iteration 335, loss = 0.00657147\n",
      "Iteration 336, loss = 0.00638346\n",
      "Iteration 337, loss = 0.00644367\n",
      "Iteration 338, loss = 0.00640938\n",
      "Iteration 339, loss = 0.00645748\n",
      "Iteration 340, loss = 0.00634146\n",
      "Iteration 341, loss = 0.00631659\n",
      "Iteration 342, loss = 0.00634111\n",
      "Iteration 343, loss = 0.00629210\n",
      "Iteration 344, loss = 0.00621913\n",
      "Iteration 345, loss = 0.00646037\n",
      "Iteration 346, loss = 0.00632354\n",
      "Iteration 347, loss = 0.00644646\n",
      "Iteration 348, loss = 0.00638408\n",
      "Iteration 349, loss = 0.00625916\n",
      "Iteration 350, loss = 0.00609640\n",
      "Iteration 351, loss = 0.00610721\n",
      "Iteration 352, loss = 0.00604836\n",
      "Iteration 353, loss = 0.00602452\n",
      "Iteration 354, loss = 0.00598118\n",
      "Iteration 355, loss = 0.00595064\n",
      "Iteration 356, loss = 0.00600558\n",
      "Iteration 357, loss = 0.00598548\n",
      "Iteration 358, loss = 0.00593156\n",
      "Iteration 359, loss = 0.00605385\n",
      "Iteration 360, loss = 0.00596707\n",
      "Iteration 361, loss = 0.00584659\n",
      "Iteration 362, loss = 0.00584063\n",
      "Iteration 363, loss = 0.00584426\n",
      "Iteration 364, loss = 0.00585348\n",
      "Iteration 365, loss = 0.00585694\n",
      "Iteration 366, loss = 0.00576412\n",
      "Iteration 367, loss = 0.00570774\n",
      "Iteration 368, loss = 0.00565915\n",
      "Iteration 369, loss = 0.00561128\n",
      "Iteration 370, loss = 0.00563668\n",
      "Iteration 371, loss = 0.00555877\n",
      "Iteration 372, loss = 0.00555110\n",
      "Iteration 373, loss = 0.00560768\n",
      "Iteration 374, loss = 0.00561171\n",
      "Iteration 375, loss = 0.00551087\n",
      "Iteration 376, loss = 0.00553104\n",
      "Iteration 377, loss = 0.00559241\n",
      "Iteration 378, loss = 0.00539851\n",
      "Iteration 379, loss = 0.00550267\n",
      "Iteration 380, loss = 0.00540805\n",
      "Iteration 381, loss = 0.00539492\n",
      "Iteration 382, loss = 0.00533918\n",
      "Iteration 383, loss = 0.00532582\n",
      "Iteration 384, loss = 0.00530941\n",
      "Iteration 385, loss = 0.00538510\n",
      "Iteration 386, loss = 0.00533361\n",
      "Iteration 387, loss = 0.00518861\n",
      "Iteration 388, loss = 0.00521645\n",
      "Iteration 389, loss = 0.00522376\n",
      "Iteration 390, loss = 0.00518407\n",
      "Iteration 391, loss = 0.00518249\n",
      "Iteration 392, loss = 0.00509419\n",
      "Iteration 393, loss = 0.00517004\n",
      "Iteration 394, loss = 0.00504456\n",
      "Iteration 395, loss = 0.00509882\n",
      "Iteration 396, loss = 0.00513620\n",
      "Iteration 397, loss = 0.00500592\n",
      "Iteration 398, loss = 0.00501997\n",
      "Iteration 399, loss = 0.00496302\n",
      "Iteration 400, loss = 0.00499535\n",
      "Iteration 401, loss = 0.00502226\n",
      "Iteration 402, loss = 0.00500512\n",
      "Iteration 403, loss = 0.00490469\n",
      "Iteration 404, loss = 0.00493272\n",
      "Iteration 405, loss = 0.00489591\n",
      "Iteration 406, loss = 0.00505423\n",
      "Iteration 407, loss = 0.00478077\n",
      "Iteration 408, loss = 0.00489016\n",
      "Iteration 409, loss = 0.00484647\n",
      "Iteration 410, loss = 0.00484847\n",
      "Iteration 411, loss = 0.00474364\n",
      "Iteration 412, loss = 0.00477115\n",
      "Iteration 413, loss = 0.00479854\n",
      "Iteration 414, loss = 0.00491343\n",
      "Iteration 415, loss = 0.00476252\n",
      "Iteration 416, loss = 0.00463347\n",
      "Iteration 417, loss = 0.00468198\n",
      "Iteration 418, loss = 0.00463326\n",
      "Iteration 419, loss = 0.00469610\n",
      "Iteration 420, loss = 0.00461166\n",
      "Iteration 421, loss = 0.00473374\n",
      "Iteration 422, loss = 0.00466019\n",
      "Iteration 423, loss = 0.00461138\n",
      "Iteration 424, loss = 0.00465281\n",
      "Iteration 425, loss = 0.00456113\n",
      "Iteration 426, loss = 0.00449185\n",
      "Iteration 427, loss = 0.00447716\n",
      "Iteration 428, loss = 0.00447355\n",
      "Iteration 429, loss = 0.00444610\n",
      "Iteration 430, loss = 0.00443141\n",
      "Iteration 431, loss = 0.00442007\n",
      "Iteration 432, loss = 0.00444466\n",
      "Iteration 433, loss = 0.00439357\n",
      "Iteration 434, loss = 0.00431728\n",
      "Iteration 435, loss = 0.00438991\n",
      "Iteration 436, loss = 0.00430223\n",
      "Iteration 437, loss = 0.00425542\n",
      "Iteration 438, loss = 0.00432149\n",
      "Iteration 439, loss = 0.00438469\n",
      "Iteration 440, loss = 0.00438716\n",
      "Iteration 441, loss = 0.00430375\n",
      "Iteration 442, loss = 0.00438876\n",
      "Iteration 443, loss = 0.00422192\n",
      "Iteration 444, loss = 0.00429540\n",
      "Iteration 445, loss = 0.00420104\n",
      "Iteration 446, loss = 0.00418676\n",
      "Iteration 447, loss = 0.00440208\n",
      "Iteration 448, loss = 0.00426241\n",
      "Iteration 449, loss = 0.00416706\n",
      "Iteration 450, loss = 0.00415015\n",
      "Iteration 451, loss = 0.00410185\n",
      "Iteration 452, loss = 0.00411905\n",
      "Iteration 453, loss = 0.00414444\n",
      "Iteration 454, loss = 0.00399948\n",
      "Iteration 455, loss = 0.00403094\n",
      "Iteration 456, loss = 0.00404756\n",
      "Iteration 457, loss = 0.00394661\n",
      "Iteration 458, loss = 0.00390533\n",
      "Iteration 459, loss = 0.00398674\n",
      "Iteration 460, loss = 0.00395092\n",
      "Iteration 461, loss = 0.00398461\n",
      "Iteration 462, loss = 0.00397945\n",
      "Iteration 463, loss = 0.00391437\n",
      "Iteration 464, loss = 0.00387591\n",
      "Iteration 465, loss = 0.00397517\n",
      "Iteration 466, loss = 0.00389330\n",
      "Iteration 467, loss = 0.00387832\n",
      "Iteration 468, loss = 0.00394913\n",
      "Iteration 469, loss = 0.00382198\n",
      "Iteration 470, loss = 0.00379551\n",
      "Iteration 471, loss = 0.00382819\n",
      "Iteration 472, loss = 0.00383455\n",
      "Iteration 473, loss = 0.00378242\n",
      "Iteration 474, loss = 0.00374526\n",
      "Iteration 475, loss = 0.00367314\n",
      "Iteration 476, loss = 0.00377350\n",
      "Iteration 477, loss = 0.00372997\n",
      "Iteration 478, loss = 0.00371672\n",
      "Iteration 479, loss = 0.00367417\n",
      "Iteration 480, loss = 0.00364548\n",
      "Iteration 481, loss = 0.00359537\n",
      "Iteration 482, loss = 0.00362764\n",
      "Iteration 483, loss = 0.00361506\n",
      "Iteration 484, loss = 0.00357372\n",
      "Iteration 485, loss = 0.00363588\n",
      "Iteration 486, loss = 0.00357963\n",
      "Iteration 487, loss = 0.00356511\n",
      "Iteration 488, loss = 0.00357226\n",
      "Iteration 489, loss = 0.00360927\n",
      "Iteration 490, loss = 0.00349687\n",
      "Iteration 491, loss = 0.00348018\n",
      "Iteration 492, loss = 0.00366242\n",
      "Iteration 493, loss = 0.00350019\n",
      "Iteration 494, loss = 0.00359043\n",
      "Iteration 495, loss = 0.00342145\n",
      "Iteration 496, loss = 0.00350118\n",
      "Iteration 497, loss = 0.00343134\n",
      "Iteration 498, loss = 0.00343803\n",
      "Iteration 499, loss = 0.00341670\n",
      "Iteration 500, loss = 0.00336963\n",
      "Iteration 501, loss = 0.00341115\n",
      "Iteration 502, loss = 0.00334436\n",
      "Iteration 503, loss = 0.00334077\n",
      "Iteration 504, loss = 0.00332594\n",
      "Iteration 505, loss = 0.00330828\n",
      "Iteration 506, loss = 0.00331126\n",
      "Iteration 507, loss = 0.00335258\n",
      "Iteration 508, loss = 0.00328274\n",
      "Iteration 509, loss = 0.00333244\n",
      "Iteration 510, loss = 0.00328627\n",
      "Iteration 511, loss = 0.00326761\n",
      "Iteration 512, loss = 0.00327471\n",
      "Iteration 513, loss = 0.00328816\n",
      "Iteration 514, loss = 0.00319197\n",
      "Iteration 515, loss = 0.00320902\n",
      "Iteration 516, loss = 0.00320401\n",
      "Iteration 517, loss = 0.00317079\n",
      "Iteration 518, loss = 0.00337561\n",
      "Iteration 519, loss = 0.00311983\n",
      "Iteration 520, loss = 0.00324558\n",
      "Iteration 521, loss = 0.00313787\n",
      "Iteration 522, loss = 0.00322344\n",
      "Iteration 523, loss = 0.00318649\n",
      "Iteration 524, loss = 0.00304395\n",
      "Iteration 525, loss = 0.00324746\n",
      "Iteration 526, loss = 0.00307267\n",
      "Iteration 527, loss = 0.00311470\n",
      "Iteration 528, loss = 0.00312041\n",
      "Iteration 529, loss = 0.00302883\n",
      "Iteration 530, loss = 0.00303699\n",
      "Iteration 531, loss = 0.00306313\n",
      "Iteration 532, loss = 0.00296667\n",
      "Iteration 533, loss = 0.00317003\n",
      "Iteration 534, loss = 0.00300709\n",
      "Iteration 535, loss = 0.00292887\n",
      "Iteration 536, loss = 0.00311457\n",
      "Iteration 537, loss = 0.00298028\n",
      "Iteration 538, loss = 0.00306780\n",
      "Iteration 539, loss = 0.00312635\n",
      "Iteration 540, loss = 0.00293387\n",
      "Iteration 541, loss = 0.00295617\n",
      "Iteration 542, loss = 0.00292824\n",
      "Iteration 543, loss = 0.00292588\n",
      "Iteration 544, loss = 0.00289108\n",
      "Iteration 545, loss = 0.00289689\n",
      "Iteration 546, loss = 0.00285623\n",
      "Iteration 547, loss = 0.00288641\n",
      "Iteration 548, loss = 0.00291470\n",
      "Iteration 549, loss = 0.00282111\n",
      "Iteration 550, loss = 0.00283336\n",
      "Iteration 551, loss = 0.00280125\n",
      "Iteration 552, loss = 0.00291301\n",
      "Iteration 553, loss = 0.00284548\n",
      "Iteration 554, loss = 0.00298631\n",
      "Iteration 555, loss = 0.00264908\n",
      "Iteration 556, loss = 0.00301198\n",
      "Iteration 557, loss = 0.00275142\n",
      "Iteration 558, loss = 0.00280354\n",
      "Iteration 559, loss = 0.00273342\n",
      "Iteration 560, loss = 0.00270261\n",
      "Iteration 561, loss = 0.00271593\n",
      "Iteration 562, loss = 0.00267786\n",
      "Iteration 563, loss = 0.00268719\n",
      "Iteration 564, loss = 0.00266312\n",
      "Iteration 565, loss = 0.00266577\n",
      "Iteration 566, loss = 0.00287038\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instanciando e treinando o modelo\n",
    "# 3 -> 100 -> 100 -> 1\n",
    "# 3 -> 2 -> 2 -> 1\n",
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
    "                                   solver = 'adam', activation = 'relu',\n",
    "                                   hidden_layer_sizes = (20,20))\n",
    "rede_neural_credit.fit(X_train_credit, y_train_credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gothic-conservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando as predi√ß√µes\n",
    "y_pred = rede_neural_credit.predict(X_test_credit)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hindu-hardware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores reais\n",
    "y_test_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-tongue",
   "metadata": {},
   "source": [
    "### An√°lise dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reserved-investor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_credit, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bearing-cambridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[435,   1],\n",
       "       [  1,  63]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_credit, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "racial-central",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "nR99mSzDxtN3",
    "outputId": "febb93b5-5921-4921-f208-62bca7218409"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_train_credit, y_train_credit)\n",
    "cm.score(X_test_credit, y_test_credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "armed-worse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVWcUg5QyS9a",
    "outputId": "8add061d-8452-49ec-8c99-54a10d5c0811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      0.99      0.99       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_credit, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "laughing-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_credit, y_train_credit, X_test_credit, y_test_credit\n",
    "del rede_neural_credit, y_pred, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-paris",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "written-transcription",
   "metadata": {
    "id": "lnC-s4bLnRmt"
   },
   "source": [
    "## Base census data - Resultado da an√°lise: 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-chorus",
   "metadata": {},
   "source": [
    "### Importando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pharmaceutical-elevation",
   "metadata": {
    "id": "TU2GJRlU0And"
   },
   "outputs": [],
   "source": [
    "with open('./data/census.pkl', 'rb') as f:\n",
    "    X_train_census, y_train_census, X_test_census, y_test_census = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "damaged-joseph",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11c886Pf1kwf",
    "outputId": "d2b54a4b-dc70-4960-a6da-958b599121d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24420, 108), (24420,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_census.shape, y_train_census.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "failing-oakland",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cD_tACqD1v6Y",
    "outputId": "618d9b30-8da5-480c-9337-62f0bc1ede86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8141, 108), (8141,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_census.shape, y_test_census.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-request",
   "metadata": {},
   "source": [
    "### Experimentando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "seasonal-buffer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIhQjIv-thWw",
    "outputId": "fcd932ec-0ab4-4db8-c767-769b6a2cec28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(108 + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "healthy-thomas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnU8BRlr15uT",
    "outputId": "fa82675f-9ce9-43ff-9746-26cce0f920b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40284893\n",
      "Iteration 2, loss = 0.33004850\n",
      "Iteration 3, loss = 0.31813097\n",
      "Iteration 4, loss = 0.31041257\n",
      "Iteration 5, loss = 0.30502028\n",
      "Iteration 6, loss = 0.30008820\n",
      "Iteration 7, loss = 0.29823091\n",
      "Iteration 8, loss = 0.29476187\n",
      "Iteration 9, loss = 0.29179534\n",
      "Iteration 10, loss = 0.28875405\n",
      "Iteration 11, loss = 0.28786340\n",
      "Iteration 12, loss = 0.28571859\n",
      "Iteration 13, loss = 0.28291276\n",
      "Iteration 14, loss = 0.28015603\n",
      "Iteration 15, loss = 0.27877984\n",
      "Iteration 16, loss = 0.27587934\n",
      "Iteration 17, loss = 0.27528613\n",
      "Iteration 18, loss = 0.27319308\n",
      "Iteration 19, loss = 0.27164486\n",
      "Iteration 20, loss = 0.27028857\n",
      "Iteration 21, loss = 0.26826229\n",
      "Iteration 22, loss = 0.26653443\n",
      "Iteration 23, loss = 0.26544561\n",
      "Iteration 24, loss = 0.26324613\n",
      "Iteration 25, loss = 0.26228603\n",
      "Iteration 26, loss = 0.25957383\n",
      "Iteration 27, loss = 0.25764956\n",
      "Iteration 28, loss = 0.25729521\n",
      "Iteration 29, loss = 0.25538366\n",
      "Iteration 30, loss = 0.25375388\n",
      "Iteration 31, loss = 0.25257226\n",
      "Iteration 32, loss = 0.25233572\n",
      "Iteration 33, loss = 0.24987358\n",
      "Iteration 34, loss = 0.24883342\n",
      "Iteration 35, loss = 0.24747070\n",
      "Iteration 36, loss = 0.24621383\n",
      "Iteration 37, loss = 0.24436645\n",
      "Iteration 38, loss = 0.24528757\n",
      "Iteration 39, loss = 0.24313358\n",
      "Iteration 40, loss = 0.24060555\n",
      "Iteration 41, loss = 0.23934447\n",
      "Iteration 42, loss = 0.23925678\n",
      "Iteration 43, loss = 0.23814855\n",
      "Iteration 44, loss = 0.23644014\n",
      "Iteration 45, loss = 0.23528360\n",
      "Iteration 46, loss = 0.23411727\n",
      "Iteration 47, loss = 0.23575145\n",
      "Iteration 48, loss = 0.23128611\n",
      "Iteration 49, loss = 0.23254238\n",
      "Iteration 50, loss = 0.23266150\n",
      "Iteration 51, loss = 0.22922030\n",
      "Iteration 52, loss = 0.22875300\n",
      "Iteration 53, loss = 0.22705195\n",
      "Iteration 54, loss = 0.22688214\n",
      "Iteration 55, loss = 0.22543072\n",
      "Iteration 56, loss = 0.22481966\n",
      "Iteration 57, loss = 0.22274731\n",
      "Iteration 58, loss = 0.22300518\n",
      "Iteration 59, loss = 0.22228961\n",
      "Iteration 60, loss = 0.22122883\n",
      "Iteration 61, loss = 0.21909367\n",
      "Iteration 62, loss = 0.21836949\n",
      "Iteration 63, loss = 0.21913343\n",
      "Iteration 64, loss = 0.21654989\n",
      "Iteration 65, loss = 0.21760843\n",
      "Iteration 66, loss = 0.21619233\n",
      "Iteration 67, loss = 0.21605086\n",
      "Iteration 68, loss = 0.21438093\n",
      "Iteration 69, loss = 0.21439798\n",
      "Iteration 70, loss = 0.21521175\n",
      "Iteration 71, loss = 0.21281389\n",
      "Iteration 72, loss = 0.21098923\n",
      "Iteration 73, loss = 0.21034811\n",
      "Iteration 74, loss = 0.20969150\n",
      "Iteration 75, loss = 0.20838010\n",
      "Iteration 76, loss = 0.20966397\n",
      "Iteration 77, loss = 0.20715715\n",
      "Iteration 78, loss = 0.20666341\n",
      "Iteration 79, loss = 0.20599406\n",
      "Iteration 80, loss = 0.20441399\n",
      "Iteration 81, loss = 0.20584645\n",
      "Iteration 82, loss = 0.20433418\n",
      "Iteration 83, loss = 0.20308575\n",
      "Iteration 84, loss = 0.20356692\n",
      "Iteration 85, loss = 0.20252292\n",
      "Iteration 86, loss = 0.20264696\n",
      "Iteration 87, loss = 0.20142139\n",
      "Iteration 88, loss = 0.19984346\n",
      "Iteration 89, loss = 0.19982368\n",
      "Iteration 90, loss = 0.19892132\n",
      "Iteration 91, loss = 0.19715169\n",
      "Iteration 92, loss = 0.19798018\n",
      "Iteration 93, loss = 0.19836695\n",
      "Iteration 94, loss = 0.19846183\n",
      "Iteration 95, loss = 0.19599836\n",
      "Iteration 96, loss = 0.19714563\n",
      "Iteration 97, loss = 0.19707267\n",
      "Iteration 98, loss = 0.19404946\n",
      "Iteration 99, loss = 0.19392744\n",
      "Iteration 100, loss = 0.19316963\n",
      "Iteration 101, loss = 0.19182120\n",
      "Iteration 102, loss = 0.19100248\n",
      "Iteration 103, loss = 0.19109972\n",
      "Iteration 104, loss = 0.19210180\n",
      "Iteration 105, loss = 0.19087552\n",
      "Iteration 106, loss = 0.19232516\n",
      "Iteration 107, loss = 0.19061129\n",
      "Iteration 108, loss = 0.19093249\n",
      "Iteration 109, loss = 0.18871247\n",
      "Iteration 110, loss = 0.19093383\n",
      "Iteration 111, loss = 0.18805376\n",
      "Iteration 112, loss = 0.18822179\n",
      "Iteration 113, loss = 0.18669041\n",
      "Iteration 114, loss = 0.18493963\n",
      "Iteration 115, loss = 0.18529470\n",
      "Iteration 116, loss = 0.18592330\n",
      "Iteration 117, loss = 0.18532449\n",
      "Iteration 118, loss = 0.18433107\n",
      "Iteration 119, loss = 0.18414766\n",
      "Iteration 120, loss = 0.18455330\n",
      "Iteration 121, loss = 0.18239614\n",
      "Iteration 122, loss = 0.18386615\n",
      "Iteration 123, loss = 0.18271948\n",
      "Iteration 124, loss = 0.18160745\n",
      "Iteration 125, loss = 0.18065329\n",
      "Iteration 126, loss = 0.18099220\n",
      "Iteration 127, loss = 0.18032327\n",
      "Iteration 128, loss = 0.18312430\n",
      "Iteration 129, loss = 0.17968480\n",
      "Iteration 130, loss = 0.18163892\n",
      "Iteration 131, loss = 0.17896525\n",
      "Iteration 132, loss = 0.18149234\n",
      "Iteration 133, loss = 0.17773443\n",
      "Iteration 134, loss = 0.17832717\n",
      "Iteration 135, loss = 0.17800306\n",
      "Iteration 136, loss = 0.17638215\n",
      "Iteration 137, loss = 0.17724461\n",
      "Iteration 138, loss = 0.17589752\n",
      "Iteration 139, loss = 0.17745615\n",
      "Iteration 140, loss = 0.17523664\n",
      "Iteration 141, loss = 0.17524208\n",
      "Iteration 142, loss = 0.17360513\n",
      "Iteration 143, loss = 0.17556751\n",
      "Iteration 144, loss = 0.17360035\n",
      "Iteration 145, loss = 0.17492148\n",
      "Iteration 146, loss = 0.17448479\n",
      "Iteration 147, loss = 0.17362692\n",
      "Iteration 148, loss = 0.17358754\n",
      "Iteration 149, loss = 0.17295246\n",
      "Iteration 150, loss = 0.17235532\n",
      "Iteration 151, loss = 0.17307960\n",
      "Iteration 152, loss = 0.17011939\n",
      "Iteration 153, loss = 0.17108448\n",
      "Iteration 154, loss = 0.16950159\n",
      "Iteration 155, loss = 0.17089888\n",
      "Iteration 156, loss = 0.17053399\n",
      "Iteration 157, loss = 0.16917428\n",
      "Iteration 158, loss = 0.16847888\n",
      "Iteration 159, loss = 0.17000467\n",
      "Iteration 160, loss = 0.16908441\n",
      "Iteration 161, loss = 0.16977292\n",
      "Iteration 162, loss = 0.16816424\n",
      "Iteration 163, loss = 0.16698913\n",
      "Iteration 164, loss = 0.16829865\n",
      "Iteration 165, loss = 0.16620111\n",
      "Iteration 166, loss = 0.16882465\n",
      "Iteration 167, loss = 0.17177510\n",
      "Iteration 168, loss = 0.16582150\n",
      "Iteration 169, loss = 0.16540338\n",
      "Iteration 170, loss = 0.16557362\n",
      "Iteration 171, loss = 0.16428163\n",
      "Iteration 172, loss = 0.16413267\n",
      "Iteration 173, loss = 0.16554732\n",
      "Iteration 174, loss = 0.16604392\n",
      "Iteration 175, loss = 0.16426389\n",
      "Iteration 176, loss = 0.16520804\n",
      "Iteration 177, loss = 0.16288599\n",
      "Iteration 178, loss = 0.16279942\n",
      "Iteration 179, loss = 0.16312869\n",
      "Iteration 180, loss = 0.16425954\n",
      "Iteration 181, loss = 0.16451816\n",
      "Iteration 182, loss = 0.16226316\n",
      "Iteration 183, loss = 0.16200291\n",
      "Iteration 184, loss = 0.16035665\n",
      "Iteration 185, loss = 0.16032090\n",
      "Iteration 186, loss = 0.16241763\n",
      "Iteration 187, loss = 0.16182939\n",
      "Iteration 188, loss = 0.16023491\n",
      "Iteration 189, loss = 0.16076012\n",
      "Iteration 190, loss = 0.16295221\n",
      "Iteration 191, loss = 0.15919247\n",
      "Iteration 192, loss = 0.15894440\n",
      "Iteration 193, loss = 0.16033694\n",
      "Iteration 194, loss = 0.15903549\n",
      "Iteration 195, loss = 0.15818830\n",
      "Iteration 196, loss = 0.15927212\n",
      "Iteration 197, loss = 0.15841618\n",
      "Iteration 198, loss = 0.15783106\n",
      "Iteration 199, loss = 0.15848518\n",
      "Iteration 200, loss = 0.15781396\n",
      "Iteration 201, loss = 0.15975141\n",
      "Iteration 202, loss = 0.15617172\n",
      "Iteration 203, loss = 0.15706661\n",
      "Iteration 204, loss = 0.15929444\n",
      "Iteration 205, loss = 0.15686168\n",
      "Iteration 206, loss = 0.15793617\n",
      "Iteration 207, loss = 0.15589482\n",
      "Iteration 208, loss = 0.15759483\n",
      "Iteration 209, loss = 0.15413061\n",
      "Iteration 210, loss = 0.15472100\n",
      "Iteration 211, loss = 0.15538204\n",
      "Iteration 212, loss = 0.15392995\n",
      "Iteration 213, loss = 0.15475071\n",
      "Iteration 214, loss = 0.15456781\n",
      "Iteration 215, loss = 0.15380822\n",
      "Iteration 216, loss = 0.15368063\n",
      "Iteration 217, loss = 0.15284773\n",
      "Iteration 218, loss = 0.15691076\n",
      "Iteration 219, loss = 0.15365476\n",
      "Iteration 220, loss = 0.15247998\n",
      "Iteration 221, loss = 0.15151980\n",
      "Iteration 222, loss = 0.15314596\n",
      "Iteration 223, loss = 0.15269157\n",
      "Iteration 224, loss = 0.15180481\n",
      "Iteration 225, loss = 0.15329431\n",
      "Iteration 226, loss = 0.15070958\n",
      "Iteration 227, loss = 0.15200786\n",
      "Iteration 228, loss = 0.15296114\n",
      "Iteration 229, loss = 0.15086375\n",
      "Iteration 230, loss = 0.15005719\n",
      "Iteration 231, loss = 0.15112928\n",
      "Iteration 232, loss = 0.15094860\n",
      "Iteration 233, loss = 0.15045188\n",
      "Iteration 234, loss = 0.14867766\n",
      "Iteration 235, loss = 0.15073303\n",
      "Iteration 236, loss = 0.15210333\n",
      "Iteration 237, loss = 0.14959297\n",
      "Iteration 238, loss = 0.14868563\n",
      "Iteration 239, loss = 0.15121458\n",
      "Iteration 240, loss = 0.14824805\n",
      "Iteration 241, loss = 0.14821503\n",
      "Iteration 242, loss = 0.14907223\n",
      "Iteration 243, loss = 0.14766413\n",
      "Iteration 244, loss = 0.14754526\n",
      "Iteration 245, loss = 0.14821025\n",
      "Iteration 246, loss = 0.14866007\n",
      "Iteration 247, loss = 0.15022380\n",
      "Iteration 248, loss = 0.14895906\n",
      "Iteration 249, loss = 0.15127306\n",
      "Iteration 250, loss = 0.14659759\n",
      "Iteration 251, loss = 0.14816413\n",
      "Iteration 252, loss = 0.14736281\n",
      "Iteration 253, loss = 0.14637008\n",
      "Iteration 254, loss = 0.14778665\n",
      "Iteration 255, loss = 0.14488920\n",
      "Iteration 256, loss = 0.14634958\n",
      "Iteration 257, loss = 0.14539968\n",
      "Iteration 258, loss = 0.14311578\n",
      "Iteration 259, loss = 0.14473446\n",
      "Iteration 260, loss = 0.14358845\n",
      "Iteration 261, loss = 0.14564376\n",
      "Iteration 262, loss = 0.14686008\n",
      "Iteration 263, loss = 0.14419537\n",
      "Iteration 264, loss = 0.14404314\n",
      "Iteration 265, loss = 0.14431029\n",
      "Iteration 266, loss = 0.14501721\n",
      "Iteration 267, loss = 0.14323559\n",
      "Iteration 268, loss = 0.14262585\n",
      "Iteration 269, loss = 0.14326698\n",
      "Iteration 270, loss = 0.14385957\n",
      "Iteration 271, loss = 0.14369955\n",
      "Iteration 272, loss = 0.14169466\n",
      "Iteration 273, loss = 0.14301377\n",
      "Iteration 274, loss = 0.14143655\n",
      "Iteration 275, loss = 0.14443049\n",
      "Iteration 276, loss = 0.14353012\n",
      "Iteration 277, loss = 0.14121385\n",
      "Iteration 278, loss = 0.14148680\n",
      "Iteration 279, loss = 0.14221221\n",
      "Iteration 280, loss = 0.14008002\n",
      "Iteration 281, loss = 0.14120293\n",
      "Iteration 282, loss = 0.14092421\n",
      "Iteration 283, loss = 0.14179949\n",
      "Iteration 284, loss = 0.14090292\n",
      "Iteration 285, loss = 0.14290619\n",
      "Iteration 286, loss = 0.14044625\n",
      "Iteration 287, loss = 0.14177037\n",
      "Iteration 288, loss = 0.14008562\n",
      "Iteration 289, loss = 0.13799819\n",
      "Iteration 290, loss = 0.14130468\n",
      "Iteration 291, loss = 0.14053990\n",
      "Iteration 292, loss = 0.14026322\n",
      "Iteration 293, loss = 0.13798255\n",
      "Iteration 294, loss = 0.14404146\n",
      "Iteration 295, loss = 0.14025965\n",
      "Iteration 296, loss = 0.13999528\n",
      "Iteration 297, loss = 0.13995688\n",
      "Iteration 298, loss = 0.13863629\n",
      "Iteration 299, loss = 0.14136423\n",
      "Iteration 300, loss = 0.14048091\n",
      "Iteration 301, loss = 0.13743758\n",
      "Iteration 302, loss = 0.13739740\n",
      "Iteration 303, loss = 0.14014125\n",
      "Iteration 304, loss = 0.13874466\n",
      "Iteration 305, loss = 0.13799039\n",
      "Iteration 306, loss = 0.13673641\n",
      "Iteration 307, loss = 0.13839616\n",
      "Iteration 308, loss = 0.13622795\n",
      "Iteration 309, loss = 0.13723828\n",
      "Iteration 310, loss = 0.13729406\n",
      "Iteration 311, loss = 0.14035791\n",
      "Iteration 312, loss = 0.13976149\n",
      "Iteration 313, loss = 0.13478850\n",
      "Iteration 314, loss = 0.13579011\n",
      "Iteration 315, loss = 0.13371839\n",
      "Iteration 316, loss = 0.13916625\n",
      "Iteration 317, loss = 0.14001082\n",
      "Iteration 318, loss = 0.13752299\n",
      "Iteration 319, loss = 0.13632067\n",
      "Iteration 320, loss = 0.13505402\n",
      "Iteration 321, loss = 0.13438931\n",
      "Iteration 322, loss = 0.13474777\n",
      "Iteration 323, loss = 0.13481461\n",
      "Iteration 324, loss = 0.13599978\n",
      "Iteration 325, loss = 0.13385586\n",
      "Iteration 326, loss = 0.13693517\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 108 -> 55 -> 55 -> 1\n",
    "rede_neural_census = MLPClassifier(verbose=True, max_iter = 1000, tol=0.000010,\n",
    "                                  hidden_layer_sizes = (55,55))\n",
    "rede_neural_census.fit(X_train_census, y_train_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "coated-still",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' >50K', ' <=50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rede_neural_census.predict(X_test_census)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "individual-spectrum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' >50K', ' <=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_census"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-joint",
   "metadata": {},
   "source": [
    "### An√°lise dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "anticipated-disability",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JAtM61v2eiA",
    "outputId": "96eb41d4-c095-46e7-b520-1e64234a361f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8198010072472669"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_census, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sufficient-frost",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "L0K6x5C-3B3e",
    "outputId": "abbd4ac1-162e-4cbf-9aeb-efa2e2f3b593"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8198010072472669"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_train_census, y_train_census)\n",
    "cm.score(X_test_census, y_test_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "laden-knock",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73BnWAg83fkr",
    "outputId": "3ed86fac-ddbf-4d64-9fda-305150cac49d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.90      0.88      6159\n",
      "        >50K       0.64      0.58      0.61      1982\n",
      "\n",
      "    accuracy                           0.82      8141\n",
      "   macro avg       0.76      0.74      0.75      8141\n",
      "weighted avg       0.81      0.82      0.82      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_census, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aggregate-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_census, y_train_census, X_test_census, y_test_census\n",
    "del rede_neural_census, y_pred, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-algorithm",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
